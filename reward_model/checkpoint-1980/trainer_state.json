{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 2000,
  "global_step": 1980,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 0.06130475550889969,
      "learning_rate": 9.999773426770863e-07,
      "loss": 0.8859,
      "step": 10
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 0.1460367888212204,
      "learning_rate": 9.998990237032888e-07,
      "loss": 0.8373,
      "step": 20
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.08102775365114212,
      "learning_rate": 9.997647721194234e-07,
      "loss": 1.0638,
      "step": 30
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 0.06821376085281372,
      "learning_rate": 9.99574602946607e-07,
      "loss": 0.943,
      "step": 40
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 0.08203782141208649,
      "learning_rate": 9.993285374624529e-07,
      "loss": 0.8645,
      "step": 50
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.06256614625453949,
      "learning_rate": 9.990266031986908e-07,
      "loss": 0.8053,
      "step": 60
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 0.15316025912761688,
      "learning_rate": 9.98668833938086e-07,
      "loss": 0.9288,
      "step": 70
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 0.07861462980508804,
      "learning_rate": 9.9825526971066e-07,
      "loss": 0.7997,
      "step": 80
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.052502695471048355,
      "learning_rate": 9.97785956789211e-07,
      "loss": 0.8949,
      "step": 90
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 0.07719365507364273,
      "learning_rate": 9.972609476841365e-07,
      "loss": 1.0234,
      "step": 100
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.10888129472732544,
      "learning_rate": 9.966803011375594e-07,
      "loss": 1.0174,
      "step": 110
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.14317427575588226,
      "learning_rate": 9.96044082116753e-07,
      "loss": 0.9043,
      "step": 120
    },
    {
      "epoch": 1.3131313131313131,
      "grad_norm": 0.08858897536993027,
      "learning_rate": 9.953523618068748e-07,
      "loss": 0.7833,
      "step": 130
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 0.14437083899974823,
      "learning_rate": 9.946052176029993e-07,
      "loss": 0.8624,
      "step": 140
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 0.11070366948843002,
      "learning_rate": 9.9380273310146e-07,
      "loss": 0.8649,
      "step": 150
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 0.11271047592163086,
      "learning_rate": 9.929449980904951e-07,
      "loss": 0.909,
      "step": 160
    },
    {
      "epoch": 1.7171717171717171,
      "grad_norm": 0.17769412696361542,
      "learning_rate": 9.920321085402022e-07,
      "loss": 0.913,
      "step": 170
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.0884067714214325,
      "learning_rate": 9.91064166591799e-07,
      "loss": 0.8652,
      "step": 180
    },
    {
      "epoch": 1.9191919191919191,
      "grad_norm": 0.10340946167707443,
      "learning_rate": 9.900412805461966e-07,
      "loss": 0.8399,
      "step": 190
    },
    {
      "epoch": 2.0202020202020203,
      "grad_norm": 0.1357976496219635,
      "learning_rate": 9.889635648518809e-07,
      "loss": 0.9149,
      "step": 200
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 0.1116330698132515,
      "learning_rate": 9.878311400921072e-07,
      "loss": 0.8265,
      "step": 210
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.06828245520591736,
      "learning_rate": 9.866441329714087e-07,
      "loss": 0.7799,
      "step": 220
    },
    {
      "epoch": 2.323232323232323,
      "grad_norm": 0.1349988877773285,
      "learning_rate": 9.854026763014204e-07,
      "loss": 0.9184,
      "step": 230
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 0.14758770167827606,
      "learning_rate": 9.841069089860174e-07,
      "loss": 0.9919,
      "step": 240
    },
    {
      "epoch": 2.525252525252525,
      "grad_norm": 0.09631062299013138,
      "learning_rate": 9.827569760057754e-07,
      "loss": 0.8664,
      "step": 250
    },
    {
      "epoch": 2.6262626262626263,
      "grad_norm": 0.22983665764331818,
      "learning_rate": 9.813530284017472e-07,
      "loss": 0.9132,
      "step": 260
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.10737758874893188,
      "learning_rate": 9.798952232585645e-07,
      "loss": 0.8313,
      "step": 270
    },
    {
      "epoch": 2.8282828282828283,
      "grad_norm": 0.13433533906936646,
      "learning_rate": 9.783837236868609e-07,
      "loss": 0.8628,
      "step": 280
    },
    {
      "epoch": 2.929292929292929,
      "grad_norm": 0.13552968204021454,
      "learning_rate": 9.768186988050226e-07,
      "loss": 0.8927,
      "step": 290
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 0.12734490633010864,
      "learning_rate": 9.752003237202662e-07,
      "loss": 0.9611,
      "step": 300
    },
    {
      "epoch": 3.1313131313131315,
      "grad_norm": 0.13825634121894836,
      "learning_rate": 9.735287795090454e-07,
      "loss": 0.8446,
      "step": 310
    },
    {
      "epoch": 3.2323232323232323,
      "grad_norm": 0.23537927865982056,
      "learning_rate": 9.718042531967916e-07,
      "loss": 0.892,
      "step": 320
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.09206435084342957,
      "learning_rate": 9.700269377369881e-07,
      "loss": 0.9775,
      "step": 330
    },
    {
      "epoch": 3.4343434343434343,
      "grad_norm": 0.10759123414754868,
      "learning_rate": 9.681970319895802e-07,
      "loss": 0.9277,
      "step": 340
    },
    {
      "epoch": 3.5353535353535355,
      "grad_norm": 0.08727900683879852,
      "learning_rate": 9.663147406987258e-07,
      "loss": 0.8109,
      "step": 350
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.09079199284315109,
      "learning_rate": 9.643802744698865e-07,
      "loss": 0.8855,
      "step": 360
    },
    {
      "epoch": 3.7373737373737375,
      "grad_norm": 0.16426776349544525,
      "learning_rate": 9.623938497462645e-07,
      "loss": 0.8951,
      "step": 370
    },
    {
      "epoch": 3.8383838383838382,
      "grad_norm": 0.1218041181564331,
      "learning_rate": 9.60355688784584e-07,
      "loss": 0.9146,
      "step": 380
    },
    {
      "epoch": 3.9393939393939394,
      "grad_norm": 0.1054316982626915,
      "learning_rate": 9.58266019630224e-07,
      "loss": 0.8102,
      "step": 390
    },
    {
      "epoch": 4.040404040404041,
      "grad_norm": 0.07120473682880402,
      "learning_rate": 9.561250760917025e-07,
      "loss": 0.8213,
      "step": 400
    },
    {
      "epoch": 4.141414141414141,
      "grad_norm": 0.15179884433746338,
      "learning_rate": 9.53933097714517e-07,
      "loss": 0.8917,
      "step": 410
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 0.13845166563987732,
      "learning_rate": 9.516903297543409e-07,
      "loss": 0.9838,
      "step": 420
    },
    {
      "epoch": 4.343434343434343,
      "grad_norm": 0.21687468886375427,
      "learning_rate": 9.493970231495834e-07,
      "loss": 0.9252,
      "step": 430
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.11062552034854889,
      "learning_rate": 9.470534344933126e-07,
      "loss": 0.92,
      "step": 440
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 0.10707499831914902,
      "learning_rate": 9.446598260045449e-07,
      "loss": 0.9164,
      "step": 450
    },
    {
      "epoch": 4.646464646464646,
      "grad_norm": 0.13388462364673615,
      "learning_rate": 9.422164654989071e-07,
      "loss": 0.9157,
      "step": 460
    },
    {
      "epoch": 4.747474747474747,
      "grad_norm": 0.16417257487773895,
      "learning_rate": 9.397236263586703e-07,
      "loss": 0.9177,
      "step": 470
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 0.1648666262626648,
      "learning_rate": 9.371815875021618e-07,
      "loss": 0.8208,
      "step": 480
    },
    {
      "epoch": 4.94949494949495,
      "grad_norm": 0.10544924437999725,
      "learning_rate": 9.34590633352558e-07,
      "loss": 1.023,
      "step": 490
    },
    {
      "epoch": 5.05050505050505,
      "grad_norm": 0.15106457471847534,
      "learning_rate": 9.319510538060605e-07,
      "loss": 0.9315,
      "step": 500
    },
    {
      "epoch": 5.151515151515151,
      "grad_norm": 0.10074196755886078,
      "learning_rate": 9.292631441994603e-07,
      "loss": 0.845,
      "step": 510
    },
    {
      "epoch": 5.252525252525253,
      "grad_norm": 0.14072708785533905,
      "learning_rate": 9.265272052770935e-07,
      "loss": 0.8839,
      "step": 520
    },
    {
      "epoch": 5.353535353535354,
      "grad_norm": 0.1334623545408249,
      "learning_rate": 9.23743543157191e-07,
      "loss": 0.8385,
      "step": 530
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 0.1451333463191986,
      "learning_rate": 9.209124692976287e-07,
      "loss": 0.8659,
      "step": 540
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 0.1294037252664566,
      "learning_rate": 9.180343004610779e-07,
      "loss": 0.8648,
      "step": 550
    },
    {
      "epoch": 5.656565656565657,
      "grad_norm": 0.1541251242160797,
      "learning_rate": 9.151093586795644e-07,
      "loss": 0.8473,
      "step": 560
    },
    {
      "epoch": 5.757575757575758,
      "grad_norm": 0.14099276065826416,
      "learning_rate": 9.121379712184365e-07,
      "loss": 0.9119,
      "step": 570
    },
    {
      "epoch": 5.858585858585858,
      "grad_norm": 0.1067734807729721,
      "learning_rate": 9.091204705397483e-07,
      "loss": 0.7868,
      "step": 580
    },
    {
      "epoch": 5.959595959595959,
      "grad_norm": 0.1746162325143814,
      "learning_rate": 9.06057194265061e-07,
      "loss": 0.8804,
      "step": 590
    },
    {
      "epoch": 6.0606060606060606,
      "grad_norm": 0.08951489627361298,
      "learning_rate": 9.029484851376672e-07,
      "loss": 0.761,
      "step": 600
    },
    {
      "epoch": 6.161616161616162,
      "grad_norm": 0.1462966799736023,
      "learning_rate": 8.997946909842424e-07,
      "loss": 0.8551,
      "step": 610
    },
    {
      "epoch": 6.262626262626263,
      "grad_norm": 0.13898566365242004,
      "learning_rate": 8.965961646759272e-07,
      "loss": 0.9684,
      "step": 620
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.14513914287090302,
      "learning_rate": 8.933532640888448e-07,
      "loss": 0.8932,
      "step": 630
    },
    {
      "epoch": 6.4646464646464645,
      "grad_norm": 0.12845417857170105,
      "learning_rate": 8.900663520640603e-07,
      "loss": 0.8553,
      "step": 640
    },
    {
      "epoch": 6.565656565656566,
      "grad_norm": 0.19427435100078583,
      "learning_rate": 8.86735796366982e-07,
      "loss": 0.8443,
      "step": 650
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.09790031611919403,
      "learning_rate": 8.833619696462134e-07,
      "loss": 0.8971,
      "step": 660
    },
    {
      "epoch": 6.767676767676767,
      "grad_norm": 0.17789438366889954,
      "learning_rate": 8.799452493918585e-07,
      "loss": 0.8729,
      "step": 670
    },
    {
      "epoch": 6.8686868686868685,
      "grad_norm": 0.08458229899406433,
      "learning_rate": 8.76486017893285e-07,
      "loss": 0.9905,
      "step": 680
    },
    {
      "epoch": 6.96969696969697,
      "grad_norm": 0.21745404601097107,
      "learning_rate": 8.729846621963508e-07,
      "loss": 0.8495,
      "step": 690
    },
    {
      "epoch": 7.070707070707071,
      "grad_norm": 0.09449709206819534,
      "learning_rate": 8.694415740600988e-07,
      "loss": 0.8649,
      "step": 700
    },
    {
      "epoch": 7.171717171717171,
      "grad_norm": 0.17337284982204437,
      "learning_rate": 8.658571499129226e-07,
      "loss": 0.9731,
      "step": 710
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.20166592299938202,
      "learning_rate": 8.622317908082126e-07,
      "loss": 0.9187,
      "step": 720
    },
    {
      "epoch": 7.373737373737374,
      "grad_norm": 0.10995235294103622,
      "learning_rate": 8.585659023794817e-07,
      "loss": 0.9355,
      "step": 730
    },
    {
      "epoch": 7.474747474747475,
      "grad_norm": 0.15073269605636597,
      "learning_rate": 8.548598947949805e-07,
      "loss": 0.9071,
      "step": 740
    },
    {
      "epoch": 7.575757575757576,
      "grad_norm": 0.10982145369052887,
      "learning_rate": 8.511141827118045e-07,
      "loss": 1.0162,
      "step": 750
    },
    {
      "epoch": 7.6767676767676765,
      "grad_norm": 0.1916172355413437,
      "learning_rate": 8.473291852294986e-07,
      "loss": 0.9876,
      "step": 760
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 0.15498143434524536,
      "learning_rate": 8.435053258431653e-07,
      "loss": 0.7666,
      "step": 770
    },
    {
      "epoch": 7.878787878787879,
      "grad_norm": 0.09331908077001572,
      "learning_rate": 8.396430323960806e-07,
      "loss": 0.958,
      "step": 780
    },
    {
      "epoch": 7.97979797979798,
      "grad_norm": 0.17020447552204132,
      "learning_rate": 8.357427370318238e-07,
      "loss": 0.8642,
      "step": 790
    },
    {
      "epoch": 8.080808080808081,
      "grad_norm": 0.10944120585918427,
      "learning_rate": 8.318048761459255e-07,
      "loss": 0.7612,
      "step": 800
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 0.12203595787286758,
      "learning_rate": 8.278298903370406e-07,
      "loss": 0.8731,
      "step": 810
    },
    {
      "epoch": 8.282828282828282,
      "grad_norm": 0.08102822303771973,
      "learning_rate": 8.238182243576511e-07,
      "loss": 0.8121,
      "step": 820
    },
    {
      "epoch": 8.383838383838384,
      "grad_norm": 0.13866110146045685,
      "learning_rate": 8.197703270643017e-07,
      "loss": 0.9038,
      "step": 830
    },
    {
      "epoch": 8.484848484848484,
      "grad_norm": 0.07913072407245636,
      "learning_rate": 8.156866513673814e-07,
      "loss": 0.9018,
      "step": 840
    },
    {
      "epoch": 8.585858585858587,
      "grad_norm": 0.18920323252677917,
      "learning_rate": 8.115676541804455e-07,
      "loss": 0.9213,
      "step": 850
    },
    {
      "epoch": 8.686868686868687,
      "grad_norm": 0.17618322372436523,
      "learning_rate": 8.074137963690942e-07,
      "loss": 0.8586,
      "step": 860
    },
    {
      "epoch": 8.787878787878787,
      "grad_norm": 0.18139547109603882,
      "learning_rate": 8.032255426994068e-07,
      "loss": 0.913,
      "step": 870
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 0.2862943410873413,
      "learning_rate": 7.990033617859395e-07,
      "loss": 0.927,
      "step": 880
    },
    {
      "epoch": 8.98989898989899,
      "grad_norm": 0.07625038176774979,
      "learning_rate": 7.947477260392947e-07,
      "loss": 0.8499,
      "step": 890
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 0.1542121022939682,
      "learning_rate": 7.904591116132624e-07,
      "loss": 0.8185,
      "step": 900
    },
    {
      "epoch": 9.191919191919192,
      "grad_norm": 0.18849007785320282,
      "learning_rate": 7.861379983515448e-07,
      "loss": 0.8734,
      "step": 910
    },
    {
      "epoch": 9.292929292929292,
      "grad_norm": 0.1279837042093277,
      "learning_rate": 7.817848697340685e-07,
      "loss": 0.9045,
      "step": 920
    },
    {
      "epoch": 9.393939393939394,
      "grad_norm": 0.1360548436641693,
      "learning_rate": 7.774002128228879e-07,
      "loss": 0.8882,
      "step": 930
    },
    {
      "epoch": 9.494949494949495,
      "grad_norm": 0.12397656589746475,
      "learning_rate": 7.729845182076895e-07,
      "loss": 0.8971,
      "step": 940
    },
    {
      "epoch": 9.595959595959595,
      "grad_norm": 0.13898934423923492,
      "learning_rate": 7.685382799509006e-07,
      "loss": 0.9461,
      "step": 950
    },
    {
      "epoch": 9.696969696969697,
      "grad_norm": 0.1506556272506714,
      "learning_rate": 7.640619955324101e-07,
      "loss": 0.8383,
      "step": 960
    },
    {
      "epoch": 9.797979797979798,
      "grad_norm": 0.1751711666584015,
      "learning_rate": 7.59556165793906e-07,
      "loss": 0.8953,
      "step": 970
    },
    {
      "epoch": 9.8989898989899,
      "grad_norm": 0.1999288946390152,
      "learning_rate": 7.550212948828376e-07,
      "loss": 0.8558,
      "step": 980
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.10904747247695923,
      "learning_rate": 7.504578901960081e-07,
      "loss": 0.8457,
      "step": 990
    },
    {
      "epoch": 10.1010101010101,
      "grad_norm": 0.20367662608623505,
      "learning_rate": 7.45866462322802e-07,
      "loss": 0.9292,
      "step": 1000
    },
    {
      "epoch": 10.202020202020202,
      "grad_norm": 0.13443118333816528,
      "learning_rate": 7.41247524988057e-07,
      "loss": 0.9357,
      "step": 1010
    },
    {
      "epoch": 10.303030303030303,
      "grad_norm": 0.13831983506679535,
      "learning_rate": 7.366015949945847e-07,
      "loss": 0.8235,
      "step": 1020
    },
    {
      "epoch": 10.404040404040405,
      "grad_norm": 0.2127094566822052,
      "learning_rate": 7.319291921653463e-07,
      "loss": 0.9424,
      "step": 1030
    },
    {
      "epoch": 10.505050505050505,
      "grad_norm": 0.1339414119720459,
      "learning_rate": 7.272308392852904e-07,
      "loss": 0.8747,
      "step": 1040
    },
    {
      "epoch": 10.606060606060606,
      "grad_norm": 0.08826947212219238,
      "learning_rate": 7.225070620428604e-07,
      "loss": 0.8792,
      "step": 1050
    },
    {
      "epoch": 10.707070707070708,
      "grad_norm": 0.1303514987230301,
      "learning_rate": 7.177583889711762e-07,
      "loss": 0.8716,
      "step": 1060
    },
    {
      "epoch": 10.808080808080808,
      "grad_norm": 0.18173931539058685,
      "learning_rate": 7.129853513888971e-07,
      "loss": 0.9225,
      "step": 1070
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 0.09821917116641998,
      "learning_rate": 7.08188483340775e-07,
      "loss": 0.8889,
      "step": 1080
    },
    {
      "epoch": 11.01010101010101,
      "grad_norm": 0.1106206551194191,
      "learning_rate": 7.033683215379002e-07,
      "loss": 0.912,
      "step": 1090
    },
    {
      "epoch": 11.11111111111111,
      "grad_norm": 0.16191720962524414,
      "learning_rate": 6.985254052976501e-07,
      "loss": 0.8615,
      "step": 1100
    },
    {
      "epoch": 11.212121212121213,
      "grad_norm": 0.14708666503429413,
      "learning_rate": 6.936602764833473e-07,
      "loss": 0.8347,
      "step": 1110
    },
    {
      "epoch": 11.313131313131313,
      "grad_norm": 0.1264629364013672,
      "learning_rate": 6.887734794436299e-07,
      "loss": 0.9795,
      "step": 1120
    },
    {
      "epoch": 11.414141414141413,
      "grad_norm": 0.13184472918510437,
      "learning_rate": 6.838655609515466e-07,
      "loss": 0.8502,
      "step": 1130
    },
    {
      "epoch": 11.515151515151516,
      "grad_norm": 0.17694588005542755,
      "learning_rate": 6.789370701433797e-07,
      "loss": 0.9142,
      "step": 1140
    },
    {
      "epoch": 11.616161616161616,
      "grad_norm": 0.20308242738246918,
      "learning_rate": 6.739885584572025e-07,
      "loss": 0.9109,
      "step": 1150
    },
    {
      "epoch": 11.717171717171716,
      "grad_norm": 0.0804959088563919,
      "learning_rate": 6.690205795711811e-07,
      "loss": 0.8283,
      "step": 1160
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 0.10998968780040741,
      "learning_rate": 6.640336893416237e-07,
      "loss": 0.8271,
      "step": 1170
    },
    {
      "epoch": 11.919191919191919,
      "grad_norm": 0.2025090605020523,
      "learning_rate": 6.590284457407875e-07,
      "loss": 0.9327,
      "step": 1180
    },
    {
      "epoch": 12.02020202020202,
      "grad_norm": 0.16651761531829834,
      "learning_rate": 6.540054087944484e-07,
      "loss": 0.839,
      "step": 1190
    },
    {
      "epoch": 12.121212121212121,
      "grad_norm": 0.1399126946926117,
      "learning_rate": 6.489651405192409e-07,
      "loss": 0.8023,
      "step": 1200
    },
    {
      "epoch": 12.222222222222221,
      "grad_norm": 0.21530966460704803,
      "learning_rate": 6.439082048597755e-07,
      "loss": 0.8363,
      "step": 1210
    },
    {
      "epoch": 12.323232323232324,
      "grad_norm": 0.16589224338531494,
      "learning_rate": 6.388351676255392e-07,
      "loss": 0.8006,
      "step": 1220
    },
    {
      "epoch": 12.424242424242424,
      "grad_norm": 0.2745125889778137,
      "learning_rate": 6.337465964275898e-07,
      "loss": 1.048,
      "step": 1230
    },
    {
      "epoch": 12.525252525252526,
      "grad_norm": 0.16826112568378448,
      "learning_rate": 6.286430606150458e-07,
      "loss": 0.9079,
      "step": 1240
    },
    {
      "epoch": 12.626262626262626,
      "grad_norm": 0.15451741218566895,
      "learning_rate": 6.235251312113843e-07,
      "loss": 0.9173,
      "step": 1250
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 0.19879096746444702,
      "learning_rate": 6.183933808505494e-07,
      "loss": 0.9751,
      "step": 1260
    },
    {
      "epoch": 12.828282828282829,
      "grad_norm": 0.19707633554935455,
      "learning_rate": 6.132483837128823e-07,
      "loss": 0.8513,
      "step": 1270
    },
    {
      "epoch": 12.929292929292929,
      "grad_norm": 0.13571257889270782,
      "learning_rate": 6.080907154608765e-07,
      "loss": 0.8642,
      "step": 1280
    },
    {
      "epoch": 13.030303030303031,
      "grad_norm": 0.14209125936031342,
      "learning_rate": 6.029209531747698e-07,
      "loss": 0.9398,
      "step": 1290
    },
    {
      "epoch": 13.131313131313131,
      "grad_norm": 0.09934058040380478,
      "learning_rate": 5.977396752879741e-07,
      "loss": 0.9234,
      "step": 1300
    },
    {
      "epoch": 13.232323232323232,
      "grad_norm": 0.1054558977484703,
      "learning_rate": 5.925474615223572e-07,
      "loss": 0.837,
      "step": 1310
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 0.18721841275691986,
      "learning_rate": 5.873448928233786e-07,
      "loss": 1.0187,
      "step": 1320
    },
    {
      "epoch": 13.434343434343434,
      "grad_norm": 0.12327351421117783,
      "learning_rate": 5.821325512950885e-07,
      "loss": 0.9229,
      "step": 1330
    },
    {
      "epoch": 13.535353535353535,
      "grad_norm": 0.16197378933429718,
      "learning_rate": 5.769110201349975e-07,
      "loss": 0.9125,
      "step": 1340
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 0.09289373457431793,
      "learning_rate": 5.716808835688241e-07,
      "loss": 0.868,
      "step": 1350
    },
    {
      "epoch": 13.737373737373737,
      "grad_norm": 0.1669071912765503,
      "learning_rate": 5.664427267851271e-07,
      "loss": 0.8723,
      "step": 1360
    },
    {
      "epoch": 13.83838383838384,
      "grad_norm": 0.13210292160511017,
      "learning_rate": 5.611971358698298e-07,
      "loss": 0.9034,
      "step": 1370
    },
    {
      "epoch": 13.93939393939394,
      "grad_norm": 0.10637723654508591,
      "learning_rate": 5.55944697740644e-07,
      "loss": 0.797,
      "step": 1380
    },
    {
      "epoch": 14.04040404040404,
      "grad_norm": 0.08528900146484375,
      "learning_rate": 5.506860000814017e-07,
      "loss": 0.7694,
      "step": 1390
    },
    {
      "epoch": 14.141414141414142,
      "grad_norm": 0.14442136883735657,
      "learning_rate": 5.454216312763e-07,
      "loss": 0.8003,
      "step": 1400
    },
    {
      "epoch": 14.242424242424242,
      "grad_norm": 0.2221192866563797,
      "learning_rate": 5.40152180344068e-07,
      "loss": 0.8519,
      "step": 1410
    },
    {
      "epoch": 14.343434343434343,
      "grad_norm": 0.1605687439441681,
      "learning_rate": 5.348782368720625e-07,
      "loss": 0.9608,
      "step": 1420
    },
    {
      "epoch": 14.444444444444445,
      "grad_norm": 0.10047628730535507,
      "learning_rate": 5.296003909503018e-07,
      "loss": 1.0322,
      "step": 1430
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 0.10114739835262299,
      "learning_rate": 5.2431923310544e-07,
      "loss": 0.8401,
      "step": 1440
    },
    {
      "epoch": 14.646464646464647,
      "grad_norm": 0.2825915217399597,
      "learning_rate": 5.19035354234695e-07,
      "loss": 0.9266,
      "step": 1450
    },
    {
      "epoch": 14.747474747474747,
      "grad_norm": 0.11027315258979797,
      "learning_rate": 5.137493455397346e-07,
      "loss": 0.8158,
      "step": 1460
    },
    {
      "epoch": 14.848484848484848,
      "grad_norm": 0.17160528898239136,
      "learning_rate": 5.084617984605281e-07,
      "loss": 0.8672,
      "step": 1470
    },
    {
      "epoch": 14.94949494949495,
      "grad_norm": 0.1418015956878662,
      "learning_rate": 5.03173304609171e-07,
      "loss": 0.7888,
      "step": 1480
    },
    {
      "epoch": 15.05050505050505,
      "grad_norm": 0.26809215545654297,
      "learning_rate": 4.97884455703691e-07,
      "loss": 0.9996,
      "step": 1490
    },
    {
      "epoch": 15.151515151515152,
      "grad_norm": 0.1810484081506729,
      "learning_rate": 4.925958435018424e-07,
      "loss": 0.9406,
      "step": 1500
    },
    {
      "epoch": 15.252525252525253,
      "grad_norm": 0.16581116616725922,
      "learning_rate": 4.873080597348947e-07,
      "loss": 0.8948,
      "step": 1510
    },
    {
      "epoch": 15.353535353535353,
      "grad_norm": 0.17390985786914825,
      "learning_rate": 4.820216960414264e-07,
      "loss": 0.9322,
      "step": 1520
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 0.22147202491760254,
      "learning_rate": 4.7673734390112666e-07,
      "loss": 0.8877,
      "step": 1530
    },
    {
      "epoch": 15.555555555555555,
      "grad_norm": 0.10742774605751038,
      "learning_rate": 4.7145559456861594e-07,
      "loss": 0.8827,
      "step": 1540
    },
    {
      "epoch": 15.656565656565657,
      "grad_norm": 0.11208934336900711,
      "learning_rate": 4.6617703900729286e-07,
      "loss": 0.8107,
      "step": 1550
    },
    {
      "epoch": 15.757575757575758,
      "grad_norm": 0.10901270061731339,
      "learning_rate": 4.6090226782321176e-07,
      "loss": 0.9372,
      "step": 1560
    },
    {
      "epoch": 15.858585858585858,
      "grad_norm": 0.16849452257156372,
      "learning_rate": 4.55631871199001e-07,
      "loss": 0.7711,
      "step": 1570
    },
    {
      "epoch": 15.95959595959596,
      "grad_norm": 0.1856885850429535,
      "learning_rate": 4.5036643882782946e-07,
      "loss": 0.9748,
      "step": 1580
    },
    {
      "epoch": 16.060606060606062,
      "grad_norm": 0.3236617147922516,
      "learning_rate": 4.451065598474259e-07,
      "loss": 0.9416,
      "step": 1590
    },
    {
      "epoch": 16.161616161616163,
      "grad_norm": 0.11468733847141266,
      "learning_rate": 4.398528227741633e-07,
      "loss": 0.8449,
      "step": 1600
    },
    {
      "epoch": 16.262626262626263,
      "grad_norm": 0.33024102449417114,
      "learning_rate": 4.3460581543720925e-07,
      "loss": 1.0074,
      "step": 1610
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 0.1408584862947464,
      "learning_rate": 4.2936612491275615e-07,
      "loss": 0.8145,
      "step": 1620
    },
    {
      "epoch": 16.464646464646464,
      "grad_norm": 0.10516239702701569,
      "learning_rate": 4.241343374583342e-07,
      "loss": 0.9207,
      "step": 1630
    },
    {
      "epoch": 16.565656565656564,
      "grad_norm": 0.16895416378974915,
      "learning_rate": 4.1891103844721634e-07,
      "loss": 0.8215,
      "step": 1640
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 0.1854085922241211,
      "learning_rate": 4.136968123029221e-07,
      "loss": 0.8489,
      "step": 1650
    },
    {
      "epoch": 16.767676767676768,
      "grad_norm": 0.16206836700439453,
      "learning_rate": 4.084922424338276e-07,
      "loss": 0.9296,
      "step": 1660
    },
    {
      "epoch": 16.86868686868687,
      "grad_norm": 0.10498286038637161,
      "learning_rate": 4.0329791116789004e-07,
      "loss": 0.8551,
      "step": 1670
    },
    {
      "epoch": 16.96969696969697,
      "grad_norm": 0.14584575593471527,
      "learning_rate": 3.9811439968749083e-07,
      "loss": 0.8987,
      "step": 1680
    },
    {
      "epoch": 17.07070707070707,
      "grad_norm": 0.1477060616016388,
      "learning_rate": 3.9294228796440986e-07,
      "loss": 0.8968,
      "step": 1690
    },
    {
      "epoch": 17.171717171717173,
      "grad_norm": 0.15510031580924988,
      "learning_rate": 3.8778215469493264e-07,
      "loss": 0.9091,
      "step": 1700
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 0.14116229116916656,
      "learning_rate": 3.8263457723510195e-07,
      "loss": 0.7747,
      "step": 1710
    },
    {
      "epoch": 17.373737373737374,
      "grad_norm": 0.17408345639705658,
      "learning_rate": 3.7750013153611827e-07,
      "loss": 1.0087,
      "step": 1720
    },
    {
      "epoch": 17.474747474747474,
      "grad_norm": 0.13172176480293274,
      "learning_rate": 3.723793920798979e-07,
      "loss": 0.9451,
      "step": 1730
    },
    {
      "epoch": 17.575757575757574,
      "grad_norm": 0.1721339076757431,
      "learning_rate": 3.672729318147961e-07,
      "loss": 0.9355,
      "step": 1740
    },
    {
      "epoch": 17.67676767676768,
      "grad_norm": 0.11085790395736694,
      "learning_rate": 3.621813220915004e-07,
      "loss": 0.8959,
      "step": 1750
    },
    {
      "epoch": 17.77777777777778,
      "grad_norm": 0.19991548359394073,
      "learning_rate": 3.5710513259910325e-07,
      "loss": 0.8702,
      "step": 1760
    },
    {
      "epoch": 17.87878787878788,
      "grad_norm": 0.11869756132364273,
      "learning_rate": 3.5204493130136137e-07,
      "loss": 0.9052,
      "step": 1770
    },
    {
      "epoch": 17.97979797979798,
      "grad_norm": 0.11928680539131165,
      "learning_rate": 3.470012843731476e-07,
      "loss": 1.0086,
      "step": 1780
    },
    {
      "epoch": 18.08080808080808,
      "grad_norm": 0.17599448561668396,
      "learning_rate": 3.4197475613710195e-07,
      "loss": 0.8416,
      "step": 1790
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 0.11393266916275024,
      "learning_rate": 3.369659090004917e-07,
      "loss": 0.939,
      "step": 1800
    },
    {
      "epoch": 18.282828282828284,
      "grad_norm": 0.20861245691776276,
      "learning_rate": 3.319753033922849e-07,
      "loss": 0.8853,
      "step": 1810
    },
    {
      "epoch": 18.383838383838384,
      "grad_norm": 0.19184772670269012,
      "learning_rate": 3.270034977004441e-07,
      "loss": 0.8032,
      "step": 1820
    },
    {
      "epoch": 18.484848484848484,
      "grad_norm": 0.22672243416309357,
      "learning_rate": 3.220510482094506e-07,
      "loss": 0.9871,
      "step": 1830
    },
    {
      "epoch": 18.585858585858585,
      "grad_norm": 0.1297822743654251,
      "learning_rate": 3.1711850903806276e-07,
      "loss": 0.9324,
      "step": 1840
    },
    {
      "epoch": 18.686868686868685,
      "grad_norm": 0.2535538375377655,
      "learning_rate": 3.122064320773165e-07,
      "loss": 0.8733,
      "step": 1850
    },
    {
      "epoch": 18.78787878787879,
      "grad_norm": 0.15387557446956635,
      "learning_rate": 3.073153669287759e-07,
      "loss": 0.8102,
      "step": 1860
    },
    {
      "epoch": 18.88888888888889,
      "grad_norm": 0.14932438731193542,
      "learning_rate": 3.02445860843039e-07,
      "loss": 0.9426,
      "step": 1870
    },
    {
      "epoch": 18.98989898989899,
      "grad_norm": 0.09437686949968338,
      "learning_rate": 2.9759845865850785e-07,
      "loss": 0.8535,
      "step": 1880
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 0.1433894783258438,
      "learning_rate": 2.927737027404268e-07,
      "loss": 0.9545,
      "step": 1890
    },
    {
      "epoch": 19.19191919191919,
      "grad_norm": 0.10842737555503845,
      "learning_rate": 2.8797213292019924e-07,
      "loss": 0.797,
      "step": 1900
    },
    {
      "epoch": 19.292929292929294,
      "grad_norm": 0.14569050073623657,
      "learning_rate": 2.8319428643498627e-07,
      "loss": 0.9552,
      "step": 1910
    },
    {
      "epoch": 19.393939393939394,
      "grad_norm": 0.14000973105430603,
      "learning_rate": 2.7844069786759725e-07,
      "loss": 0.9033,
      "step": 1920
    },
    {
      "epoch": 19.494949494949495,
      "grad_norm": 0.08186861127614975,
      "learning_rate": 2.73711899086676e-07,
      "loss": 0.8892,
      "step": 1930
    },
    {
      "epoch": 19.595959595959595,
      "grad_norm": 0.11585667729377747,
      "learning_rate": 2.690084191871912e-07,
      "loss": 0.8209,
      "step": 1940
    },
    {
      "epoch": 19.696969696969695,
      "grad_norm": 0.1430830955505371,
      "learning_rate": 2.6433078443123694e-07,
      "loss": 0.9265,
      "step": 1950
    },
    {
      "epoch": 19.7979797979798,
      "grad_norm": 0.18690022826194763,
      "learning_rate": 2.5967951818915136e-07,
      "loss": 0.923,
      "step": 1960
    },
    {
      "epoch": 19.8989898989899,
      "grad_norm": 0.13856539130210876,
      "learning_rate": 2.550551408809565e-07,
      "loss": 0.9858,
      "step": 1970
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.14795039594173431,
      "learning_rate": 2.5045816991813057e-07,
      "loss": 0.9514,
      "step": 1980
    }
  ],
  "logging_steps": 10,
  "max_steps": 2970,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
